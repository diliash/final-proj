{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycocotools.mask as mask_util\n",
    "\n",
    "import torch\n",
    "\n",
    "def normalize(tensor, mean, std):\n",
    "    mask = torch.any(tensor > 0, dim=0, keepdim=True)\n",
    "\n",
    "    # Normalize only non-black pixels\n",
    "    normalized_tensor = torch.zeros_like(tensor)\n",
    "    normalized_tensor = torch.where(\n",
    "        mask,\n",
    "        (tensor - mean.view(-1, 1, 1)) / std.view(-1, 1, 1),\n",
    "        normalized_tensor\n",
    "    )\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, coco_folder, image_mean=[0, 0, 0], image_std=[0, 0, 0], train=True):\n",
    "        ann_file = os.path.join(coco_folder, \"coco_annotation\", \"MotionNet_train.json\" if train else \"MotionNet_valid.json\")\n",
    "        super(CocoDetection, self).__init__(os.path.join(coco_folder, \"train/origin\" if train else \"valid/origin\"), ann_file)\n",
    "        self.pixel_mean = torch.tensor(image_mean)\n",
    "        self.pixel_std = torch.tensor(image_std)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        segms = [ann[\"segmentation\"] for ann in target[\"annotations\"]]\n",
    "        masks = []\n",
    "        for segm in segms:\n",
    "            if segm:  # Check if the segmentation is not empty\n",
    "                rle = mask_util.frPyObjects(segm, 1024, 1024)\n",
    "                rle = mask_util.merge(rle)\n",
    "                mask = mask_util.decode(rle).astype(bool)\n",
    "                mask = Image.fromarray(mask)\n",
    "                masks.append(mask)\n",
    "        \n",
    "        # Filter out instances without masks\n",
    "        if not masks:\n",
    "            return None\n",
    "        \n",
    "        # Does not work well on practice\n",
    "        #img = normalize(img, self.pixel_mean, self.pixel_std)\n",
    "        return img, target, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "image_mean = [123.675 / 255, 116.28 / 255, 103.53 / 255]\n",
    "image_std = [58.395 / 255, 57.12 / 255, 57.375 / 255]\n",
    "\n",
    "train_dataset = CocoDetection(coco_folder='./partnetsim-1024-fixed-viewpoints/coco', image_mean=image_mean, image_std=image_std, train=True)\n",
    "val_dataset = CocoDetection(coco_folder='./partnetsim-1024-fixed-viewpoints/coco', image_mean=image_mean, image_std=image_std, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoProcessor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor.image_processor.do_normalize = False\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return None\n",
    "    imgs = [item[0] for item in batch]\n",
    "    bboxes = [[ann[\"bbox\"] for ann in item[1][\"annotations\"]] for item in batch]\n",
    "    masks = [item[2] for item in batch]\n",
    "\n",
    "    augmented_bboxes = []\n",
    "    for bbox_list in bboxes:\n",
    "        augmented_bbox_list = []\n",
    "        for bbox in bbox_list:\n",
    "            x_min, y_min, w, h = bbox\n",
    "            x_max = x_min + w\n",
    "            y_max = y_min + h\n",
    "            \n",
    "            img_width, img_height = imgs[0].size\n",
    "            \n",
    "            # Apply random variations to bbox coordinates\n",
    "            # I have tried 0-70 and 0-20\n",
    "            x_min = max(0, x_min - random.randint(0, 70))\n",
    "            x_max = min(img_width, x_max + random.randint(0, 70))\n",
    "            y_min = max(0, y_min - random.randint(0, 70))\n",
    "            y_max = min(img_height, y_max + random.randint(0, 70))\n",
    "            \n",
    "            x_new = x_min\n",
    "            y_new = y_min\n",
    "            w_new = x_max - x_min\n",
    "            h_new = y_max - y_min\n",
    "            \n",
    "            augmented_bbox_list.append([x_new, y_new, w_new, h_new])\n",
    "        \n",
    "        augmented_bboxes.append(augmented_bbox_list)\n",
    "\n",
    "    # Pad bboxes to have the same length within a batch\n",
    "    max_bboxes = max(len(bbox_list) for bbox_list in augmented_bboxes)\n",
    "    padded_bboxes = [bbox_list + [[0, 0, 0, 0]] * (max_bboxes - len(bbox_list)) for bbox_list in augmented_bboxes]\n",
    "\n",
    "    input = processor(images=imgs, input_boxes=padded_bboxes, return_tensors=\"pt\").to(torch.float32)\n",
    "\n",
    "    masks = [torch.tensor(np.array(mask), dtype=torch.int) for mask in masks]\n",
    "    masks = pad_sequence(masks, batch_first=True).to(device)\n",
    "    input[\"masks\"] = masks\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import SamModel, SamProcessor\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = inputs.float()  \n",
    "        targets = targets.float()  \n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        intersection = torch.sum(inputs * targets, dim=(1, 2))\n",
    "        union = torch.sum(inputs, dim=(1, 2)) + torch.sum(targets, dim=(1, 2))\n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "class SAM(pl.LightningModule):\n",
    "    def __init__(self, lr, lr_backbone, weight_decay, processor, model_name):\n",
    "        super().__init__()\n",
    "        self.model = SamModel.from_pretrained(model_name)\n",
    "        # Output only one mask per image\n",
    "        self.model.multimask_output = False\n",
    "        self.processor = processor\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "        self.focal_loss = FocalLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "\n",
    "    def forward(self, pixel_values, bboxes):\n",
    "        outputs = self.model(pixel_values=pixel_values, input_boxes=bboxes)\n",
    "        return outputs\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        if batch is not None:\n",
    "            gt_masks = batch.pop(\"masks\")\n",
    "            outputs = self.model(**batch, multimask_output=False)\n",
    "\n",
    "            # Interpolate masks fro 256 to 1024\n",
    "            masks = F.interpolate(\n",
    "                outputs[\"pred_masks\"].squeeze(2),\n",
    "                (1024, 1024),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            \n",
    "            masks = masks.float() \n",
    "            gt_masks = gt_masks.float()  \n",
    "    \n",
    "            loss_focal = self.focal_loss(masks, gt_masks)\n",
    "            loss_dice = self.dice_loss(masks, gt_masks)\n",
    "\n",
    "            # Compute IoU loss\n",
    "            pred_masks = (masks >= 0.5).float()\n",
    "            intersection = torch.sum(torch.mul(pred_masks, gt_masks), dim=(1, 2))\n",
    "            union = torch.sum(pred_masks, dim=(1, 2)) + torch.sum(gt_masks, dim=(1, 2))\n",
    "            epsilon = 1e-7\n",
    "            batch_iou = (intersection / (union + epsilon)).unsqueeze(1)\n",
    "    \n",
    "            iou_scores = outputs.iou_scores.view(-1, 1)\n",
    "            batch_iou = batch_iou.view(-1, 1)\n",
    "            min_len = min(iou_scores.size(0), batch_iou.size(0))\n",
    "            iou_scores = iou_scores[:min_len]\n",
    "            batch_iou = batch_iou[:min_len]\n",
    "            loss_iou = F.mse_loss(iou_scores, batch_iou, reduction='mean')\n",
    "    \n",
    "            loss_total = 20. * loss_focal + loss_dice + loss_iou\n",
    "    \n",
    "            return loss_total, {\"focal_loss\": loss_focal, \"dice_loss\": loss_dice, \"iou_loss\": loss_iou}\n",
    "        else:\n",
    "            # Case of empty batch (no masks)\n",
    "            return None, {}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        if loss is not None:\n",
    "            self.log(\"training_loss\", loss)\n",
    "            for k, v in loss_dict.items():\n",
    "                self.log(\"train_\" + k, v.item())\n",
    "            return loss\n",
    "        else:\n",
    "            self.log(\"training_loss\", 0)\n",
    "            for k, v in loss_dict.items():\n",
    "                self.log(\"train_\" + k, v.item())\n",
    "            return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        if loss is not None:\n",
    "            self.log(\"validation_loss\", loss)\n",
    "            for k, v in loss_dict.items():\n",
    "                self.log(\"validation_\" + k, v.item())\n",
    "            return loss\n",
    "        else:\n",
    "            self.log(\"validation_loss\", 0)\n",
    "            for k, v in loss_dict.items():\n",
    "                self.log(\"validation_\" + k, v.item())\n",
    "            return 0\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Decoder and Encoder get different learning rates\n",
    "        param_dicts = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"decoder\" in n and p.requires_grad],\n",
    "                \"lr\": self.lr,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"shared_image_embedding\" in n and p.requires_grad],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for GPUs with tensor cores\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"SAM-pl-finetune\")\n",
    "\n",
    "dirpath = f\"checkpoints/SAM/{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "model = SAM(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4, processor=processor, model_name=\"facebook/sam-vit-base\").to(device)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=3000,\n",
    "    gradient_clip_val=0.2,\n",
    "    logger=wandb_logger,\n",
    "    accelerator=device,\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "wandb_logger.log_hyperparams(model.hparams)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.push_to_hub(f\"diliash/sam-{dirpath.split('/')[-1]}\")\n",
    "processor.push_to_hub(f\"diliash/sam-{dirpath.split('/')[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
